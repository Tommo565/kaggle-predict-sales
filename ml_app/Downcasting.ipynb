{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries & Parameters\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import gcp_token, bq_db, project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GCP credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from BQ\n",
    "\n",
    "We'll start by importing a df from Google Big Query. The Date field is transformed as BigQuery converts everything to UTC format which interferes with Pandas datetime operations.\n",
    "\n",
    "TODO: Change this to a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our SQL query\n",
    "query = f'SELECT * FROM {bq_db}.merged'\n",
    "\n",
    "# Import the data from BQ\n",
    "df = pd.read_gbq(query=query, project_id=project_id)\n",
    "\n",
    "# Transform the date column\n",
    "df['date'] = df['date'].dt.tz_localize(None)\n",
    "\n",
    "# Show the df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas will generally default to 64-bit storage for data. This means that it's more versatile at the expense of taking up memory, which in turn generally makes things longer to process.\n",
    "\n",
    "Converting this to a more efficient storage mechanism for data is called **Downcasting**. Downcasting is very quick to do and can give a welcome efficiency boost when dealing with large dataframes.\n",
    "\n",
    "We can check how much memory is being used with the `memory_usage()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check the data types with the `dtypes()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downcasting a Dataframe\n",
    "\n",
    "We can downcast our dataframe programatically using `dtype` and `astype` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dc = df.copy()\n",
    "\n",
    "# Get float & int cols\n",
    "float_cols = [col for col in df_dc if df_dc[col].dtype in [\"float32\", \"float64\"]]\n",
    "int_cols = [col for col in df_dc if df_dc[col].dtype in [\"int64\", \"int32\"]]\n",
    "\n",
    "# Downcast float & int cols to 16 bit numpy formats\n",
    "df_dc[float_cols] = df_dc[float_cols].astype(np.float16)\n",
    "df_dc[int_cols] = df_dc[int_cols].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dc.memory_usage().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "* [Advanced Pandas optimize Speed & Memory](https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2)\n",
    "* [Downcasting in Pandas](https://medium.com/@vincentteyssier/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-predict-sales",
   "language": "python",
   "name": "kaggle-predict-sales"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
