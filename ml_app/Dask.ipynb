{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries & Parameters\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import gcp_token, bq_db, project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GCP credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is an HPC (High Performance Computing) library for Python, based on Pandas. It was created by Continuum, the same people responsible for the Anaconda distribution of Python that we call know and love!\n",
    "\n",
    "It can either work on your personal machine or a VM or a cluster of VMs.\n",
    "\n",
    "It's quite easy to get started with and you don't need to learn a new API or language to use it.\n",
    "\n",
    "There are two ways in which it operates:\n",
    "* Distributed Data\n",
    "* Parallel Processing\n",
    "\n",
    "#### Distributed Data\n",
    "Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don't fit into main memory. Dask's high-level collections are alternatives to NumPy and Pandas for large datasets. This is very similar to how Spark Dataframes operate.\n",
    "\n",
    "#### Parallel Processing\n",
    "Dask provides dynamic task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads. These schedulers are low-latency (around 1ms) and work hard to run computations in a small memory footprint. Dask's schedulers are an alternative to direct use of threading or multiprocessing libraries in complex cases or other task scheduling systems like Luigi or IPython parallel.\n",
    "\n",
    "The above was shamelessly stolen from the [Dask Tutorial](https://github.com/dask/dask-tutorial).\n",
    "\n",
    "This tutorial focuses on **Parallel Processing**.\n",
    "\n",
    "We'll start by initialising the Dask `Client()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import multiprocessing\n",
    "\n",
    "print(f'Processors: {multiprocessing.cpu_count()}')\n",
    "\n",
    "n_workers = 4\n",
    "threads_per_worker = 3\n",
    "\n",
    "client = Client(\n",
    "    n_workers=n_workers,\n",
    "    threads_per_worker=threads_per_worker,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in `multiprocessing` library to work out how many processors our machine has available and fill this value automatically. Generally 3 - 4 threads per worker (processor) is a good balance. \n",
    "\n",
    "With 3 threads, this gives us 24 'cores'. This means instead of doing 1 thing at a time, our machine can now do 24 things at a time. Cool!\n",
    "\n",
    "Calling `client` brings up some information including a link to the dashboard. his shows us the status of the processing that Dask is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Delayed\n",
    "\n",
    "Again, this is shamelessly stolen from the [Dask Tutorial](https://github.com/dask/dask-tutorial).\n",
    "\n",
    "We can start by looking at how Python operates normally with a couple of functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run these as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes three seconds to run because we call each function sequentially, one after the other\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has caluclated `x` then `y` then `z` giving us a 3 second runtime.\n",
    "\n",
    "We can parallelise this using Dask's **delayed** Higher Order Function. A Higher Order Function is a function that takes a function as an argument, or returns a function. You can read a good explanation of this concept [here](https://www.tutorialspoint.com/functional_programming/functional_programming_higher_order_functions.htm).\n",
    "\n",
    "We can delay our functions as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = delayed(inc)(1)\n",
    "y = delayed(inc)(2)\n",
    "z = delayed(add)(x, y)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that instead of an output, we've instead got: `Delayed('add-df0666a2-e0b0-474d-a3aa-9377ac421f7a')`\n",
    "\n",
    "This is because when we delay a function, we don't actually execute it. All Dask has done at this point is make a plan for how it's going to execute our functions.\n",
    "\n",
    "We can check out this plan (a graph) by calling the `visualize()` method on our output `z` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the functions we have to call the `compute()` function on our `z` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "z = compute(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that has run in 2 seconds, a second quicker because Dask has executed `x` & `y` at the same time in parallel using different workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed in Practice: Parallelising groupby\n",
    "\n",
    "One very obvious use case for Dask is to improve the speed at which `groupby` runs, as it has a warranted reputation for running very slowly and inefficiently.\n",
    "\n",
    "We'll import some BigQuery data: \n",
    "\n",
    "TODO: Change this to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our SQL query\n",
    "query = f'SELECT * FROM {bq_db}.merged'\n",
    "\n",
    "# Import the data from BQ\n",
    "df = pd.read_gbq(query=query, project_id=project_id)\n",
    "\n",
    "# Transform the date column\n",
    "df['date'] = df['date'].dt.tz_localize(None)\n",
    "\n",
    "# Show the df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take create a sample DataFrame of the top 500 records in the BigQuery Data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('target', ascending=False)\n",
    "\n",
    "uid_list = df['item_price'].unique().tolist()[:300]\n",
    "\n",
    "df_sm = df[\n",
    "    df['item_price'].isin(uid_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a time series for each of our 100 top records using Facebook's prophet library.\n",
    "\n",
    "We'll start by making a delayed function to build an individual time series from a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "def create_single_time_series(df):\n",
    "    \"\"\"\n",
    "    Creates a Time Series using Facebook Prophet. \n",
    "    \n",
    "    Applied via groupby.\n",
    "    \"\"\"\n",
    "    \n",
    "    history = df[df['y'].notnull()].copy()\n",
    "    \n",
    "    # Only execute if there's more than 1 record\n",
    "    if history.shape[0] >= 2:\n",
    "        # Get the UID to append to the predictions\n",
    "        uid_value = df_ts['item_id'].unique().tolist()[0]\n",
    "\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=False,\n",
    "            daily_seasonality=False,\n",
    "            n_changepoints=1\n",
    "        )\n",
    "        \n",
    "        # Add monthly seasonality\n",
    "        model.add_seasonality(\n",
    "            name='monthly',\n",
    "            period=30.5,\n",
    "            fourier_order=5\n",
    "        )\n",
    "    \n",
    "        model.fit(df, iter=1000)\n",
    "\n",
    "        # Make a future dataframe to put predictions into\n",
    "        df_future = model.make_future_dataframe(periods=1)\n",
    "\n",
    "        # Generate predictions\n",
    "        df_preds = model.predict(df_future)\n",
    "\n",
    "        # Add UID\n",
    "        df_preds['item_id'] = uid_value\n",
    "\n",
    "        # Return as a dict since it's quicker to combine dicts than dataframe\n",
    "        dict_preds = df_preds.to_dict(orient='records')\n",
    "        \n",
    "    else:\n",
    "        dict_preds = []\n",
    "    \n",
    "    return dict_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly Prophet is very good for programatically generating lots of time series at once. There are both Python and R implementations and you can check out an overview [here](https://facebook.github.io/prophet/docs/quick_start.html).\n",
    "\n",
    "Secondly note the `@delayed` decorator. This gives exactly the same outcome as calling `delayed()` on a function and is Python's way of specifying a Higher Order Function.\n",
    "\n",
    "Lastly, note that we transform our dataframe to a dictionary. This is because it's quicker and easier to make a new dataframe from 100 dictionaries than it is to concatanate 100 dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 300 Time Series in Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Only keep necessary columns and rename time series and target to 'ds' and 'y'\n",
    "df_ts = (\n",
    "    df_sm[['item_id', 'date', 'target']]\n",
    "    .rename({\n",
    "        'date': 'ds',\n",
    "        'target': 'y'\n",
    "    }, axis=1)\n",
    ")\n",
    "\n",
    "# Fill any NaN values\n",
    "df_ts['y'] = df_ts['y'].fillna(0)\n",
    "\n",
    "# Create an empty list to hold our results\n",
    "ts_output = []\n",
    "\n",
    "# Group the DataFrame by unique id\n",
    "df_ts_gp = df_ts.groupby('item_id')\n",
    "\n",
    "for group in df_ts_gp.groups:\n",
    "    # Get the group (note that returned df is ungrouped)\n",
    "    df_ts = df_ts_gp.get_group(group)\n",
    "    ts_dict = delayed(create_single_time_series)(df_ts)\n",
    "    ts_output.append(ts_dict)\n",
    "    \n",
    "# Execute the function\n",
    "ts_output = compute(ts_output)[0]\n",
    "    \n",
    "df_list = []\n",
    "\n",
    "for item in ts_output:\n",
    "    for record in item:\n",
    "        df_list.append(record)\n",
    "        \n",
    "df_dask = pd.DataFrame(df_list).sort_values(['item_id', 'ds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 300 Time Series in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Only keep necessary columns and rename time series and target to 'ds' and 'y'\n",
    "df_ts = (\n",
    "    df_sm[['item_id', 'date', 'target']]\n",
    "    .rename({\n",
    "        'date': 'ds',\n",
    "        'target': 'y'\n",
    "    }, axis=1)\n",
    ")\n",
    "\n",
    "# Fill any NaN values\n",
    "df_ts['y'] = df_ts['y'].fillna(0)\n",
    "\n",
    "# Create an empty list to hold our results\n",
    "ts_output = []\n",
    "\n",
    "# Group the DataFrame by unique id\n",
    "df_ts_gp = df_ts.groupby('item_id')\n",
    "\n",
    "for group in df_ts_gp.groups:\n",
    "    # Get the group (note that returned df is ungrouped)\n",
    "    df_ts = df_ts_gp.get_group(group)\n",
    "    ts_dict = create_single_time_series(df_ts)\n",
    "    ts_output.append(ts_dict)\n",
    "    \n",
    "# Execute the function\n",
    "ts_output = compute(ts_output)[0]\n",
    "    \n",
    "df_list = []\n",
    "\n",
    "for item in ts_output:\n",
    "    for record in item:\n",
    "        df_list.append(record)\n",
    "        \n",
    "df_dask = pd.DataFrame(df_list).sort_values(['item_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "* [Dask Site](https://dask.org/)\n",
    "* [Dask Tutorial](https://github.com/dask/dask-tutorial)\n",
    "* [Dask API Reference](https://docs.dask.org/en/latest/)\n",
    "* [Higher Order Functions](https://www.tutorialspoint.com/functional_programming/functional_programming_higher_order_functions.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-predict-sales",
   "language": "python",
   "name": "kaggle-predict-sales"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
